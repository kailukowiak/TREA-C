# TREA-C: Triple-Encoded Attention for Column-aware Time Series

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![Lightning](https://img.shields.io/badge/Lightning-2.0+-purple.svg)](https://lightning.ai/)

TREA-C (Triple-Encoded Attention for Column-aware analysis) is a PyTorch Lightning-based
library for time series analysis that handles both **numeric and categorical features**
with robust **missing value support**. The library implements a novel triple-encoded
architecture that combines value channels, mask channels, and column embeddings directly
into the model structure, eliminating NaN-related computational issues while preserving
missingness information and semantic feature understanding.

## ğŸš€ Key Features

### Core Architecture

- **Triple-Encoded Architecture**: Encodes values, missing value masks, and column
  semantics as separate channels, avoiding NaN computations
- **Multi-Modal Input Support**: Handles both numeric sensor data and time-varying
  categorical features
- **Patch-Based Processing**: Efficient temporal modeling using patch-based transformers
- **Column Embeddings**: Semantic understanding of feature names using BERT or learned
  embeddings

### Multi-Dataset Training

- **Variable Feature Schemas**: Train on datasets with different numbers and types of
  features
- **Column Semantic Embeddings**: Transfer knowledge between datasets using feature name
  semantics
- **Auto-Expanding Vocabularies**: Dynamic feature vocabularies for multi-dataset
  scenarios
- **Unified Feature Space**: Automatic padding and masking for heterogeneous datasets

### Self-Supervised Learning

- **Masked Patch Prediction**: Reconstruct masked temporal patches
- **Temporal Order Prediction**: Learn temporal relationships
- **Contrastive Learning**: Global sequence-level representations
- **Flexible SSL Objectives**: Configurable pre-training strategies

## ğŸ“¦ Installation

### Using uv (Recommended)

```bash
git clone https://github.com/kailukowiak/TREA-C.git
cd TREA-C
uv sync
```

### Using pip

```bash
git clone https://github.com/kailukowiak/TREA-C.git
cd TREA-C
pip install -e .
```

### Requirements

- Python 3.10+
- PyTorch 2.0+
- PyTorch Lightning 2.0+
- Transformers (for BERT embeddings)
- Scikit-learn, Pandas, NumPy

## ğŸ—ï¸ Package Structure

```
TREA-C/
â”œâ”€â”€ treac/                          # Core library
â”‚   â”œâ”€â”€ models/                     # Model implementations
â”‚   â”‚   â”œâ”€â”€ triple_attention.py     # Core TripleEncodedAttention
â”‚   â”‚   â”œâ”€â”€ multi_dataset_model.py  # Multi-dataset training model
â”‚   â”‚   â”œâ”€â”€ patchtstnan.py          # PatchTST with NaN handling
â”‚   â”‚   â”œâ”€â”€ embeddings.py           # Column embedding strategies
â”‚   â”‚   â””â”€â”€ ssl_objectives.py       # Self-supervised learning
â”‚   â”œâ”€â”€ utils/                      # Core utilities
â”‚   â”‚   â”œâ”€â”€ paths.py                # Path management
â”‚   â”‚   â””â”€â”€ sequence_standardization.py
â”‚   â””â”€â”€ training/                   # Training utilities
â”œâ”€â”€ utils/                          # Helper utilities (gitignored data handling)
â”‚   â”œâ”€â”€ dataset_base.py             # Base dataset classes
â”‚   â”œâ”€â”€ datamodule.py               # PyTorch Lightning data modules
â”‚   â””â”€â”€ data_config.py              # Dataset configuration
â”œâ”€â”€ data/                           # Data storage (gitignored)
â”‚   â””â”€â”€ downloaders/                # Dataset download utilities
â”œâ”€â”€ examples/                       # Usage examples and scripts
â”œâ”€â”€ tests/                          # Unit tests
â””â”€â”€ scripts/                        # Utility scripts
```

## ğŸ¯ Quick Start

### Basic Time Series Classification

```python
import torch
from treac.models import DualPatchTransformer
from utils import SyntheticTimeSeriesDataset, TimeSeriesDataModule
import pytorch_lightning as pl

# Create synthetic dataset
dataset = SyntheticTimeSeriesDataset(
    num_samples=1000,
    T=96,           # sequence length
    C_num=7,        # numeric features
    C_cat=2,        # categorical features
    num_classes=3,
    task='classification'
)

# Setup data module
dm = TimeSeriesDataModule(
    train_dataset=dataset,
    val_dataset=dataset,
    batch_size=32
)

# Create model
model = DualPatchTransformer(
    c_in=7,                    # numeric input channels
    seq_len=96,                # sequence length
    num_classes=3,             # output classes
    patch_len=16,              # patch size
    stride=8,                  # patch stride
    d_model=128,               # model dimension
    n_head=8,                  # attention heads
    num_layers=3,              # transformer layers
    task='classification'
)

# Train
trainer = pl.Trainer(max_epochs=10)
trainer.fit(model, dm)
```

### Multi-Dataset Training with Column Embeddings

```python
from treac.models import MultiDatasetModel
from data.downloaders.etth1 import ETTh1Dataset

# Load dataset
dataset = ETTh1Dataset(train=True)
column_names = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']

# Create column-aware model
model = MultiDatasetModel.create_column_aware(
    c_in=7,
    seq_len=96,
    num_classes=3,
    column_names=column_names,
    task='classification',
    patch_len=16,
    stride=8,
    d_model=128,
    n_head=8,
    num_layers=3,
    column_embedding_dim=16,
    mode='standard'
)

# The model now understands feature semantics!
```

### Self-Supervised Pretraining

```python
# Create pretraining model
pretrain_model = MultiDatasetModel(
    max_numeric_features=10,
    seq_len=96,
    patch_len=16,
    stride=8,
    d_model=128,
    n_head=8,
    num_layers=3,
    mode='pretrain',  # Enable SSL objectives

    # SSL configuration
    ssl_lambda_mask=1.0,      # Masked patch prediction weight
    ssl_lambda_temporal=0.5,   # Temporal order weight
    ssl_lambda_contrastive=0.3 # Contrastive learning weight
)

# Pretrain on unlabeled data
trainer.fit(pretrain_model, unlabeled_datamodule)

# Fine-tune on labeled data
finetuned_model = MultiDatasetModel.load_from_checkpoint(
    checkpoint_path,
    mode='standard',  # Switch to supervised mode
    num_classes=3
)
```

## ğŸ”§ Advanced Features

### Handling Missing Values

TREA-C's triple-encoded architecture automatically handles missing values:

```python
# Your data can contain NaNs - no preprocessing needed!
x_num = torch.tensor([
    [1.0, 2.0, float('nan'), 4.0],
    [float('nan'), 6.0, 7.0, 8.0]
])

# Model automatically creates value + mask channels:
# - Value channel: [1.0, 2.0, 0.0, 4.0], [0.0, 6.0, 7.0, 8.0]
# - Mask channel:  [0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0]
```

### Column Embedding Strategies

```python
from treac.models.embeddings import (
    create_column_embedding,
    create_multi_dataset_embedder
)

# Simple learned embeddings (lightweight)
simple_emb = create_column_embedding(
    column_names=['temperature', 'humidity', 'pressure'],
    target_dim=1,
    strategy='simple',
    embedding_dim=32
)

# BERT-based semantic embeddings
bert_emb = create_column_embedding(
    column_names=['temperature', 'humidity', 'pressure'],
    target_dim=1,
    strategy='bert',
    bert_model='bert-base-uncased'
)

# Multi-dataset frozen BERT (efficient for many datasets)
frozen_bert = create_multi_dataset_embedder(
    strategy='frozen_bert',
    target_dim=1
)
frozen_bert.set_columns(['temperature', 'humidity'])  # Dynamic columns
```

### Variable Feature Training

```python
# Train on datasets with different feature counts
model = MultiDatasetModel(
    max_numeric_features=20,      # Unified feature space size
    max_categorical_features=5,
    mode='variable_features'      # Enable variable schemas
)

# Set schema for each dataset
model.set_dataset_schema(
    numeric_features=7,           # This dataset has 7 features
    categorical_features=2
)

# Model automatically pads/masks features to unified space
```

## ğŸ“Š Examples and Use Cases

### Time Series Classification

- Sensor fault detection
- Human activity recognition
- Equipment state classification
- Medical time series analysis

### Multi-Dataset Scenarios

- Training on multiple sensor networks
- Transfer learning between similar datasets
- Domain adaptation for time series
- Federated learning applications

### Self-Supervised Learning

- Pretraining on large unlabeled sensor data
- Learning temporal representations
- Few-shot learning with pretrained models
- Anomaly detection via reconstruction

## ğŸ§ª Example Scripts

The `examples/` directory contains comprehensive usage examples:

- `final_patch_comparison.py` - Compare baseline vs column-aware models
- `compare_models_etth1.py` - Benchmark different architectures
- `multi_dataset_demo.py` - Multi-dataset training walkthrough
- `benchmark_patch_sizes.py` - Patch size optimization

## ğŸ”¬ Model Architecture Details

### Triple-Encoded NaN Handling

Instead of preprocessing NaNs, TREA-C triples the input channels:

- **Value channels**: Original values with NaNs â†’ 0
- **Mask channels**: 1 where NaN, 0 where valid
- **Column channels**: Semantic embeddings for each feature

This approach:

- âœ… Eliminates NaN computations (CUDA-friendly)
- âœ… Preserves missingness information
- âœ… Adds semantic feature understanding
- âœ… Allows end-to-end training
- âœ… Minimal computational overhead

### Column Embeddings

Feature names are embedded to provide semantic context:

- **Simple**: Learned embeddings (lightweight)
- **BERT**: Semantic embeddings using pre-trained language models
- **Frozen BERT**: Cached embeddings for multi-dataset efficiency
- **Auto-expanding**: Dynamic vocabularies for new features

### Multi-Dataset Training

Unified feature space with automatic padding:

```
Dataset A: [temp, humidity, pressure] â†’ [temp, humidity, pressure, 0, 0, ...]
Dataset B: [wind, rain, temp, light]  â†’ [wind, rain, temp, light, 0, ...]
```

## ğŸ“ˆ Performance

TREA-C achieves competitive performance on standard benchmarks:

- **ETTh1**: 85%+ accuracy on electricity forecasting classification
- **Multi-dataset**: Minimal performance degradation with column embeddings
- **Missing data**: Robust to 5-20% missing value rates
- **Efficiency**: ~2-3x faster than NaN-aware preprocessing

## ğŸ§ª Testing

```bash
# Run all tests
uv run pytest

# Run specific test categories
uv run pytest tests/test_models.py
uv run pytest tests/test_embeddings.py
```

## ğŸ› ï¸ Development

```bash
# Install development dependencies
uv sync --group dev

# Code formatting
uvx ruff format .

# Linting
uvx ruff check . --fix

# Type checking (if using mypy)
uvx mypy treac/
```

## ğŸ“š Citation

If you use TREA-C in your research, please cite:

```bibtex
@software{treac2024,
  title={TREA-C: Triple-Encoded Attention for Column-aware Time Series Analysis},
  author={Your Name},
  year={2024},
  url={https://github.com/kailukowiak/TREA-C}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [contributing guidelines](CONTRIBUTING.md) for
details.

## ğŸ“„ License

This project is licensed under the Apache 2.0 License - see the
[LICENSE.txt](LICENSE.txt) file for details.

## ğŸ”— Related Work

- [PatchTST](https://github.com/yuqinie98/PatchTST): Original patch-based time series
  transformer
- [TimeGPT](https://github.com/Nixtla/neuralforecast): Foundation models for time series
- [PyTorch Lightning](https://lightning.ai/): Deep learning framework used as foundation

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/kailukowiak/TREA-C/issues)
- **Discussions**:
  [GitHub Discussions](https://github.com/kailukowiak/TREA-C/discussions)
- **Documentation**: [Full Documentation](https://trea-c.readthedocs.io) (coming soon)
